{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698e1463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as  pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a906f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NUSW_NB15_Dataset(Dataset):\n",
    "    def __init__(self, path, mode='all'):\n",
    "        ds_type = path.split('/')[-1].split('-')[0]\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        if mode == 'normal':\n",
    "            # get only normal data\n",
    "            df = df[df['label'] == 0]\n",
    "        elif mode == 'anomaly':\n",
    "            # get only anormal data\n",
    "            df = df[df['label'] == 1]\n",
    "        \n",
    "        x = df.drop(['id', 'attack_cat', 'label'], axis=1)\n",
    "        y = df['label']\n",
    "\n",
    "        self.x = torch.Tensor(x.to_numpy())\n",
    "        self.y = torch.Tensor(y.to_numpy())\n",
    "\n",
    "        self.dim = self.x.shape[1]\n",
    "\n",
    "        print(\n",
    "            f'Finished reading the {ds_type} set ({mode}) of Dataset',\n",
    "            f'({len(self.x)} samples found, each dim = {self.dim})'\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "\n",
    "def prep_dataloader(path, batch_size, shuffle, mode='all'):\n",
    "    dataset = NUSW_NB15_Dataset(path, mode)\n",
    "    \n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size,\n",
    "        shuffle\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a871529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(196, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 16),\n",
    "            nn.ELU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(16, 64),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(64, 196),\n",
    "            nn.ELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97b5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn, device):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, _ in dataloader:\n",
    "            X = X.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, X).item()\n",
    "    test_loss /= num_batches\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb65a604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "# path\n",
    "tr_path = '../../data/anomaly_detection/training-set.csv'\n",
    "val_path = '../../data/anomaly_detection/validation-set.csv'\n",
    "tt_path = '../../data/anomaly_detection/testing-set.csv'\n",
    "log_path = '../../logs/anomaly_detection'\n",
    "model_path = '../../models/anomaly_detection/AE_model_weights.pth'\n",
    "\n",
    "# hyperparameter\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc28e5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading the training set (normal) of Dataset (59584 samples found, each dim = 196)\n",
      "Finished reading the validation set (normal) of Dataset (14753 samples found, each dim = 196)\n",
      "Finished reading the testing set (normal) of Dataset (18663 samples found, each dim = 196)\n",
      "Finished reading the training set (all) of Dataset (164910 samples found, each dim = 196)\n",
      "Finished reading the validation set (all) of Dataset (41228 samples found, each dim = 196)\n",
      "Finished reading the testing set (all) of Dataset (51535 samples found, each dim = 196)\n"
     ]
    }
   ],
   "source": [
    "# prepare dataloader\n",
    "normal_tr_dl = prep_dataloader(\n",
    "    tr_path,\n",
    "    batch_size,\n",
    "    shuffle=True,\n",
    "    mode='normal'\n",
    ")\n",
    "\n",
    "normal_val_dl = prep_dataloader(\n",
    "    val_path,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    mode='normal'\n",
    ")\n",
    "\n",
    "normal_tt_dl = prep_dataloader(\n",
    "    tt_path,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    mode='normal'\n",
    ")\n",
    "\n",
    "all_tr_dl = prep_dataloader(\n",
    "    tr_path,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    mode='all'\n",
    ")\n",
    "\n",
    "all_val_dl = prep_dataloader(\n",
    "    val_path,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    mode='all'\n",
    ")\n",
    "\n",
    "all_tt_dl = prep_dataloader(\n",
    "    tt_path,\n",
    "    batch_size,\n",
    "    shuffle=False,\n",
    "    mode='all'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1f806d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Load model weights form ../../models/anomaly_detection/AE_model_weights.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# neural network\n",
    "model = AE().to(device)\n",
    "\n",
    "# load model weight\n",
    "print(f'Load model weights form {model_path}')\n",
    "model.load_state_dict(torch.load(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dab56e25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11252/1549639317.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_tr_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_val_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtt_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormal_tt_dl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'avg loss'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtt_loss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'loss_fn' is not defined"
     ]
    }
   ],
   "source": [
    "tr_loss = test(normal_tr_dl, model, loss_fn, device)\n",
    "val_loss = test(normal_val_dl, model, loss_fn, device)\n",
    "tt_loss = test(normal_tt_dl, model, loss_fn, device)\n",
    "\n",
    "pd.DataFrame({'avg loss': [tr_loss, val_loss, tt_loss]})\\\n",
    "    .set_axis(['train', 'val', 'test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b0879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loss_label_df(model, dl, device):\n",
    "    losses = []\n",
    "    labels = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in dl:\n",
    "            pred = model(x.to(device)).cpu().numpy()\n",
    "            loss = np.mean(np.square(pred - x.numpy()), axis=1)\n",
    "            losses.extend(loss)\n",
    "            labels.extend(y.numpy().astype(np.int8))\n",
    "\n",
    "    return pd.DataFrame({'loss': losses, 'label': labels})\n",
    "\n",
    "\n",
    "def cal_threshold(df):\n",
    "    normal_df = df[df['label'] == 0]\n",
    "    return normal_df['loss'].mean() + normal_df['loss'].std()\n",
    "\n",
    "\n",
    "def predict(losses, threshold):\n",
    "    return list(map(lambda loss: 1 if loss > threshold else 0, losses))\n",
    "\n",
    "\n",
    "def show_confusion_matrix(df, pred):\n",
    "    conf_matrix = confusion_matrix(df['label'].values, pred)\n",
    "    sns.set(font_scale=1.2)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.heatmap(conf_matrix,\n",
    "                xticklabels=['Not Attack', 'Attack'],\n",
    "                yticklabels=['Not Attack', 'Attack'],\n",
    "                annot=True,\n",
    "                fmt=\"d\")\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel('True class')\n",
    "    plt.xlabel('Predicted class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96c0932",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_df = create_loss_label_df(model, all_tr_dl, device)\n",
    "val_df = create_loss_label_df(model, all_val_dl, device)\n",
    "tt_df = create_loss_label_df(model, all_tt_dl, device)\n",
    "\n",
    "tr_threshold = 0.00006\n",
    "print(f'threshold: {tr_threshold}\\n')\n",
    "\n",
    "tr_pred = predict(tr_df['loss'].values, tr_threshold)\n",
    "val_pred = predict(val_df['loss'].values, tr_threshold)\n",
    "tt_pred = predict(tt_df['loss'].values, tr_threshold)\n",
    "\n",
    "print('training dataset\\n')\n",
    "print(classification_report(tr_df['label'].values, tr_pred))\n",
    "\n",
    "print('validation dataset\\n')\n",
    "print(classification_report(val_df['label'].values, val_pred))\n",
    "\n",
    "print('testing dataset\\n')\n",
    "print(classification_report(tt_df['label'].values, tt_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e968f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training dataset\\n')\n",
    "show_confusion_matrix(tr_df, tr_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f9a0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('validation dataset\\n')\n",
    "show_confusion_matrix(val_df, val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cad403b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('testing dataset\\n')\n",
    "show_confusion_matrix(tt_df, tt_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
